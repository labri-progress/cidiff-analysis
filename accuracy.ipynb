{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas: 2.2.3\n",
      "Plotnine: 0.14.5\n",
      "Pingouin: 0.5.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotnine as pn\n",
    "import pingouin as pg\n",
    "\n",
    "print(\"Pandas: %s\"%pd.__version__)\n",
    "print(\"Plotnine: %s\"%pn.__version__)\n",
    "print(\"Pingouin: %s\"%pg.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = pd.read_csv('annotations.csv')\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('result.csv')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.groupby(\"path\")[\"line\"].apply(set)\n",
    "ref.head()\n",
    "#res.groupby([\"path\", \"type\"])[\"line\"].apply(set).unstack(fill_value=set())\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(human_data, algorithm_data):\n",
    "    # Group data by path for easier processing\n",
    "    human_lines = human_data.groupby(\"path\")[\"line\"].apply(set)\n",
    "    algorithm_lines = algorithm_data.groupby([\"path\", \"type\"])[\"line\"].apply(set).unstack(fill_value=set())\n",
    "    \n",
    "    # Initialize dictionaries to hold precision and recall values\n",
    "    precision_scores = {}\n",
    "    recall_scores = {}\n",
    "    \n",
    "    # Loop through each path in the human data\n",
    "    for path, human_lines_set in human_lines.items():\n",
    "        for algorithm_type in algorithm_lines.columns:\n",
    "            # Get the lines identified by the current algorithm for the current path\n",
    "            algorithm_lines_set = algorithm_lines.loc[path, algorithm_type] if path in algorithm_lines.index else set()\n",
    "            \n",
    "            # Calculate true positives, precision, and recall\n",
    "            true_positives = human_lines_set & algorithm_lines_set\n",
    "            precision = len(true_positives) / len(algorithm_lines_set) if len(algorithm_lines_set) > 0 else 0\n",
    "            recall = len(true_positives) / len(human_lines_set) if len(human_lines_set) > 0 else 0\n",
    "            \n",
    "            # Store results\n",
    "            precision_scores[(path, algorithm_type)] = precision\n",
    "            recall_scores[(path, algorithm_type)] = recall\n",
    "    \n",
    "    return precision_scores, recall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate_precision_recall(ref, res)\n",
    "accuracies = pd.DataFrame(calculate_precision_recall(ref, res)).transpose().rename(columns={0: 'precision', 1: 'recall'}).reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs = accuracies.iloc[::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cidiff = accuracies.iloc[1::2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame(calculate_precision_recall(ref, res)).transpose().rename(columns={0: 'precision', 1: 'recall'}).reindex()\n",
    "accuracies.loc[::2, 3] = 'lcs'\n",
    "accuracies.loc[1::2, 3] = 'cidiff'\n",
    "accuracies.rename(columns={3: 'algorithm'}, inplace=True)\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.ttest(accuracies[accuracies['algorithm'] == 'lcs']['precision'], accuracies[accuracies['algorithm'] == 'cidiff']['precision'],paired=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.ttest(accuracies[accuracies['algorithm'] == 'lcs']['recall'], accuracies[accuracies['algorithm'] == 'cidiff']['recall'],paired=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = pd.melt(accuracies, id_vars='algorithm', value_vars=['precision', 'recall'], var_name='metric', value_name='value')\n",
    "pn.ggplot(melted, pn.aes(x='algorithm', y='value', fill='algorithm')) + pn.geom_violin() + pn.facet_wrap('~metric', scales='free') + pn.theme(subplots_adjust={'wspace': 0.25},legend_position='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = lcs.describe().merge(cidiff.describe(), left_index=True, right_index=True, suffixes=(' lcs', ' seed')).rename(columns={'precision lcs': 'Precision LCS-diff', 'recall lcs': 'Recall LCS-diff', 'precision seed': 'Precision CiDiff', 'recall seed': 'Recall CiDiff'})\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accuracies.tex', 'w') as tf:\n",
    "     tf.write(merged.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
